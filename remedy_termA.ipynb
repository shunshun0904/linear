{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>注射時期</th>\n",
       "      <th>年/月/日</th>\n",
       "      <th>治療前視力</th>\n",
       "      <th>治療前Log</th>\n",
       "      <th>治療後視力</th>\n",
       "      <th>治療後Log</th>\n",
       "      <th>注射時期.1</th>\n",
       "      <th>年/月/日.1</th>\n",
       "      <th>治療前視力.1</th>\n",
       "      <th>治療前Log.1</th>\n",
       "      <th>...</th>\n",
       "      <th>年/月/日.2</th>\n",
       "      <th>治療前視力.2</th>\n",
       "      <th>治療前Log.2</th>\n",
       "      <th>治療後視力.2</th>\n",
       "      <th>治療後Log.2</th>\n",
       "      <th>注射時期.3</th>\n",
       "      <th>治療前視力.3</th>\n",
       "      <th>治療前Log.3</th>\n",
       "      <th>治療後視力.3</th>\n",
       "      <th>治療後Log.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>160819</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>150724</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>151120.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>...</td>\n",
       "      <td>170317.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>151218</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.045757</td>\n",
       "      <td>B</td>\n",
       "      <td>160527.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.154902</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>150904</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>160318.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>160926.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>160517</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>170509.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>170801</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>151127</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.154902</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>160711.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>160701</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>170203.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>170509</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>170403</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170821.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A</td>\n",
       "      <td>170914</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A</td>\n",
       "      <td>161024</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A</td>\n",
       "      <td>170410</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170828.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A</td>\n",
       "      <td>150310</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>160525.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>170126.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A</td>\n",
       "      <td>170529</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A</td>\n",
       "      <td>151120</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>160122.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>160620.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.045757</td>\n",
       "      <td>D</td>\n",
       "      <td>170731.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A</td>\n",
       "      <td>160115</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A</td>\n",
       "      <td>170112</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A</td>\n",
       "      <td>170807</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A</td>\n",
       "      <td>170727</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>160226.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>...</td>\n",
       "      <td>160513.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A</td>\n",
       "      <td>170831</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>161006.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>160115.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A</td>\n",
       "      <td>141203</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>B</td>\n",
       "      <td>150204.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>...</td>\n",
       "      <td>150520.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A</td>\n",
       "      <td>170224</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>170915.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A</td>\n",
       "      <td>170209</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170703.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A</td>\n",
       "      <td>160315</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>171127.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A</td>\n",
       "      <td>170410</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A</td>\n",
       "      <td>171212</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A</td>\n",
       "      <td>170724</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A</td>\n",
       "      <td>160907</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>B</td>\n",
       "      <td>170531.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>...</td>\n",
       "      <td>170927.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A</td>\n",
       "      <td>170327</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170731.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>171127.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A</td>\n",
       "      <td>150703</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>160801.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>A</td>\n",
       "      <td>150703</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>B</td>\n",
       "      <td>160822.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.795880</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>A</td>\n",
       "      <td>160822</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>A</td>\n",
       "      <td>170224</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170524.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>A</td>\n",
       "      <td>150914</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>151211.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>160311.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>A</td>\n",
       "      <td>151013</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>151208.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160216.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>A</td>\n",
       "      <td>150928</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>160222.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>160815.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>A</td>\n",
       "      <td>151030</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>160205.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A</td>\n",
       "      <td>151002</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>151211.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>...</td>\n",
       "      <td>160304.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>A</td>\n",
       "      <td>151016</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>A</td>\n",
       "      <td>151225</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>B</td>\n",
       "      <td>160527.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>A</td>\n",
       "      <td>160205</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>160610.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>161024.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>A</td>\n",
       "      <td>160318</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>A</td>\n",
       "      <td>160325</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>160815.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>170227.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>A</td>\n",
       "      <td>160415</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>A</td>\n",
       "      <td>160415</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>A</td>\n",
       "      <td>160623</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>160721.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>161006.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>A</td>\n",
       "      <td>160624</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.154902</td>\n",
       "      <td>B</td>\n",
       "      <td>160829.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.154902</td>\n",
       "      <td>...</td>\n",
       "      <td>161128.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>A</td>\n",
       "      <td>160620</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>160801.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>160905.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>A</td>\n",
       "      <td>160812</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>A</td>\n",
       "      <td>160812</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>170612.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>A</td>\n",
       "      <td>160823</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.795880</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>B</td>\n",
       "      <td>161220.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>A</td>\n",
       "      <td>161220</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170627.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>A</td>\n",
       "      <td>170417</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>B</td>\n",
       "      <td>170508.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>A</td>\n",
       "      <td>161129</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>A</td>\n",
       "      <td>161215</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.795880</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>B</td>\n",
       "      <td>170227.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>170508.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>A</td>\n",
       "      <td>170116</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.221849</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>170710.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>A</td>\n",
       "      <td>170123</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>B</td>\n",
       "      <td>170327.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>...</td>\n",
       "      <td>170529.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>A</td>\n",
       "      <td>170213</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.886057</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>B</td>\n",
       "      <td>170313.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>...</td>\n",
       "      <td>170529.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>A</td>\n",
       "      <td>171025</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   注射時期   年/月/日  治療前視力    治療前Log  治療後視力    治療後Log 注射時期.1   年/月/日.1  治療前視力.1  \\\n",
       "0     A  160819   0.60  0.221849   1.00  0.000000      B       NaN      NaN   \n",
       "1     A  150724   0.40  0.397940   0.60  0.221849      B  151120.0     0.50   \n",
       "2     A  151218   0.20  0.698970   0.90  0.045757      B  160527.0     0.70   \n",
       "3     A  150904   0.40  0.397940   0.60  0.221849      B  160318.0     0.60   \n",
       "4     A  160517   0.50  0.301030   0.60  0.221849      B  170509.0     0.50   \n",
       "5     A  170801   0.60  0.221849   1.00  0.000000    NaN       NaN      NaN   \n",
       "6     A  151127   0.70  0.154902   1.00  0.000000      B  160711.0     1.00   \n",
       "7     A  160701   0.80  0.096910   1.00  0.000000      B  170203.0     1.00   \n",
       "8     A  170509   0.80  0.096910   1.00  0.000000    NaN       NaN      NaN   \n",
       "9     A  170403   0.50  0.301030   0.80  0.096910      B  170821.0     0.60   \n",
       "10    A  170914   0.10  1.000000   0.20  0.698970    NaN       NaN      NaN   \n",
       "11    A  161024   0.80  0.096910   1.00  0.000000    NaN       NaN      NaN   \n",
       "12    A  170410   0.60  0.221849   0.80  0.096910      B  170828.0     0.80   \n",
       "13    A  150310   0.80  0.096910   1.00  0.000000      B  160525.0     0.80   \n",
       "14    A  170529   0.60  0.221849   0.80  0.096910    NaN       NaN      NaN   \n",
       "15    A  151120   0.20  0.698970   0.60  0.221849      B  160122.0     0.60   \n",
       "16    A  160115   0.60  0.221849   1.00  0.000000    NaN       NaN      NaN   \n",
       "17    A  170112   0.80  0.096910   1.00  0.000000    NaN       NaN      NaN   \n",
       "18    A  170807   0.60  0.221849   1.00  0.000000    NaN       NaN      NaN   \n",
       "19    A  170727   0.50  0.301030   0.60  0.221849      B  160226.0     0.40   \n",
       "20    A  170831   0.80  0.096910   0.80  0.096910      B  161006.0     0.80   \n",
       "21    A  141203   0.50  0.301030   0.50  0.301030      B  150204.0     0.40   \n",
       "22    A  170224   0.80  0.096910   1.00  0.000000      B  170915.0     1.00   \n",
       "23    A  170209   0.30  0.522879   0.80  0.096910      B  170703.0     0.60   \n",
       "24    A  160315   0.80  0.096910   1.00  0.000000      B  171127.0     1.00   \n",
       "25    A  170410   0.50  0.301030   0.80  0.096910    NaN       NaN      NaN   \n",
       "26    A  171212   0.80  0.096910   0.60  0.221849    NaN       NaN      NaN   \n",
       "27    A  170724   1.00  0.000000   1.00  0.000000    NaN       NaN      NaN   \n",
       "28    A  160907   0.15  0.823909   0.30  0.522879      B  170531.0     0.40   \n",
       "29    A  170327   0.80  0.096910   0.80  0.096910      B  170731.0     0.60   \n",
       "..  ...     ...    ...       ...    ...       ...    ...       ...      ...   \n",
       "53    A  150703   0.50  0.301030   0.80  0.096910      B  160801.0     0.60   \n",
       "54    A  150703   0.20  0.698970   0.25  0.602060      B  160822.0     0.16   \n",
       "55    A  160822   0.25  0.602060   0.30  0.522879    NaN       NaN      NaN   \n",
       "56    A  170224   0.80  0.096910   0.80  0.096910      B  170524.0     0.80   \n",
       "57    A  150914   0.60  0.221849   1.00  0.000000      B  151211.0     0.60   \n",
       "58    A  151013   0.60  0.221849   1.00  0.000000      B  151208.0     1.00   \n",
       "59    A  150928   0.50  0.301030   0.60  0.221849      B  160222.0     0.60   \n",
       "60    A  151030   0.80  0.096910   1.00  0.000000      B  160205.0     1.00   \n",
       "61    A  151002   0.40  0.397940   1.00  0.000000      B  151211.0     0.25   \n",
       "62    A  151016   0.80  0.096910   1.00  0.000000    NaN       NaN      NaN   \n",
       "63    A  151225   0.30  0.522879   0.40  0.397940      B  160527.0     0.25   \n",
       "64    A  160205   0.25  0.602060   1.00  0.000000      B  160610.0     1.00   \n",
       "65    A  160318   0.25  0.602060   0.60  0.221849    NaN       NaN      NaN   \n",
       "66    A  160325   0.60  0.221849   1.00  0.000000      B  160815.0     0.80   \n",
       "67    A  160415   0.60  0.221849   1.00  0.000000    NaN       NaN      NaN   \n",
       "68    A  160415   0.50  0.301030   0.50  0.301030    NaN       NaN      NaN   \n",
       "69    A  160623   0.60  0.221849   0.80  0.096910      B  160721.0     0.80   \n",
       "70    A  160624   0.60  0.221849   0.70  0.154902      B  160829.0     0.70   \n",
       "71    A  160620   0.80  0.096910   0.80  0.096910      B  160801.0     0.80   \n",
       "72    A  160812   0.80  0.096910   1.00  0.000000    NaN       NaN      NaN   \n",
       "73    A  160812   0.40  0.397940   1.00  0.000000      B  170612.0     1.00   \n",
       "74    A  160823   0.16  0.795880   0.40  0.397940      B  161220.0     0.40   \n",
       "75    A  161220   0.60  0.221849   0.80  0.096910      B  170627.0     0.80   \n",
       "76    A  170417   0.80  0.096910   0.60  0.221849      B  170508.0     0.60   \n",
       "77    A  161129   0.20  0.698970   0.30  0.522879    NaN       NaN      NaN   \n",
       "78    A  161215   0.16  0.795880   0.80  0.096910      B  170227.0     0.80   \n",
       "79    A  170116   0.60  0.221849   1.00  0.000000      B  170710.0     1.00   \n",
       "80    A  170123   0.30  0.522879   0.25  0.602060      B  170327.0     0.25   \n",
       "81    A  170213   0.13  0.886057   0.30  0.522879      B  170313.0     0.30   \n",
       "82    A  171025   0.40  0.397940   0.50  0.301030      B       NaN      NaN   \n",
       "\n",
       "    治療前Log.1    ...      年/月/日.2  治療前視力.2  治療前Log.2  治療後視力.2  治療後Log.2  \\\n",
       "0        NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "1   0.301030    ...     170317.0     0.50  0.301030      0.8  0.096910   \n",
       "2   0.154902    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "3   0.221849    ...     160926.0     0.50  0.301030      0.6  0.221849   \n",
       "4   0.301030    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "5        NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "6   0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "7   0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "8        NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "9   0.221849    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "10       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "11       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "12  0.096910    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "13  0.096910    ...     170126.0     0.80  0.096910      1.0  0.000000   \n",
       "14       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "15  0.221849    ...     160620.0     0.50  0.301030      0.9  0.045757   \n",
       "16       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "17       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "18       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "19  0.397940    ...     160513.0     0.60  0.221849      0.6  0.221849   \n",
       "20  0.096910    ...     160115.0     0.80  0.096910      1.0  0.000000   \n",
       "21  0.397940    ...     150520.0     0.50  0.301030      0.5  0.301030   \n",
       "22  0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "23  0.221849    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "24  0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "25       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "26       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "27       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "28  0.397940    ...     170927.0     0.30  0.522879      0.8  0.096910   \n",
       "29  0.221849    ...     171127.0     0.60  0.221849      NaN       NaN   \n",
       "..       ...    ...          ...      ...       ...      ...       ...   \n",
       "53  0.221849    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "54  0.795880    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "55       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "56  0.096910    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "57  0.221849    ...     160311.0     0.60  0.221849      NaN       NaN   \n",
       "58  0.000000    ...     160216.0     1.00  0.000000      1.0  0.000000   \n",
       "59  0.221849    ...     160815.0     0.60  0.221849      0.8  0.096910   \n",
       "60  0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "61  0.602060    ...     160304.0     0.20  0.698970      1.0  0.000000   \n",
       "62       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "63  0.602060    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "64  0.000000    ...     161024.0     1.00  0.000000      1.0  0.000000   \n",
       "65       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "66  0.096910    ...     170227.0     0.80  0.096910      1.0  0.000000   \n",
       "67       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "68       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "69  0.096910    ...     161006.0     0.80  0.096910      1.0  0.000000   \n",
       "70  0.154902    ...     161128.0     1.00  0.000000      1.0  0.000000   \n",
       "71  0.096910    ...     160905.0     1.00  0.000000      1.0  0.000000   \n",
       "72       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "73  0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "74  0.397940    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "75  0.096910    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "76  0.221849    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "77       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "78  0.096910    ...     170508.0     1.00  1.000000      NaN       NaN   \n",
       "79  0.000000    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "80  0.602060    ...     170529.0     0.50  0.600000      NaN       NaN   \n",
       "81  0.522879    ...     170529.0     0.16  0.600000      NaN       NaN   \n",
       "82       NaN    ...          NaN      NaN       NaN      NaN       NaN   \n",
       "\n",
       "    注射時期.3   治療前視力.3  治療前Log.3   治療後視力.3  治療後Log.3  \n",
       "0      NaN       NaN       NaN       NaN       NaN  \n",
       "1      NaN       NaN       NaN       NaN       NaN  \n",
       "2      NaN       NaN       NaN       NaN       NaN  \n",
       "3      NaN       NaN       NaN       NaN       NaN  \n",
       "4      NaN       NaN       NaN       NaN       NaN  \n",
       "5      NaN       NaN       NaN       NaN       NaN  \n",
       "6      NaN       NaN       NaN       NaN       NaN  \n",
       "7      NaN       NaN       NaN       NaN       NaN  \n",
       "8      NaN       NaN       NaN       NaN       NaN  \n",
       "9      NaN       NaN       NaN       NaN       NaN  \n",
       "10     NaN       NaN       NaN       NaN       NaN  \n",
       "11     NaN       NaN       NaN       NaN       NaN  \n",
       "12     NaN       NaN       NaN       NaN       NaN  \n",
       "13     NaN       NaN       NaN       NaN       NaN  \n",
       "14     NaN       NaN       NaN       NaN       NaN  \n",
       "15       D  170731.0       0.6  0.221849       0.8  \n",
       "16     NaN       NaN       NaN       NaN       NaN  \n",
       "17     NaN       NaN       NaN       NaN       NaN  \n",
       "18     NaN       NaN       NaN       NaN       NaN  \n",
       "19     NaN       NaN       NaN       NaN       NaN  \n",
       "20     NaN       NaN       NaN       NaN       NaN  \n",
       "21     NaN       NaN       NaN       NaN       NaN  \n",
       "22     NaN       NaN       NaN       NaN       NaN  \n",
       "23     NaN       NaN       NaN       NaN       NaN  \n",
       "24     NaN       NaN       NaN       NaN       NaN  \n",
       "25     NaN       NaN       NaN       NaN       NaN  \n",
       "26     NaN       NaN       NaN       NaN       NaN  \n",
       "27     NaN       NaN       NaN       NaN       NaN  \n",
       "28     NaN       NaN       NaN       NaN       NaN  \n",
       "29     NaN       NaN       NaN       NaN       NaN  \n",
       "..     ...       ...       ...       ...       ...  \n",
       "53     NaN       NaN       NaN       NaN       NaN  \n",
       "54     NaN       NaN       NaN       NaN       NaN  \n",
       "55     NaN       NaN       NaN       NaN       NaN  \n",
       "56     NaN       NaN       NaN       NaN       NaN  \n",
       "57     NaN       NaN       NaN       NaN       NaN  \n",
       "58     NaN       NaN       NaN       NaN       NaN  \n",
       "59     NaN       NaN       NaN       NaN       NaN  \n",
       "60     NaN       NaN       NaN       NaN       NaN  \n",
       "61     NaN       NaN       NaN       NaN       NaN  \n",
       "62     NaN       NaN       NaN       NaN       NaN  \n",
       "63     NaN       NaN       NaN       NaN       NaN  \n",
       "64     NaN       NaN       NaN       NaN       NaN  \n",
       "65     NaN       NaN       NaN       NaN       NaN  \n",
       "66     NaN       NaN       NaN       NaN       NaN  \n",
       "67     NaN       NaN       NaN       NaN       NaN  \n",
       "68     NaN       NaN       NaN       NaN       NaN  \n",
       "69     NaN       NaN       NaN       NaN       NaN  \n",
       "70     NaN       NaN       NaN       NaN       NaN  \n",
       "71     NaN       NaN       NaN       NaN       NaN  \n",
       "72     NaN       NaN       NaN       NaN       NaN  \n",
       "73     NaN       NaN       NaN       NaN       NaN  \n",
       "74     NaN       NaN       NaN       NaN       NaN  \n",
       "75     NaN       NaN       NaN       NaN       NaN  \n",
       "76     NaN       NaN       NaN       NaN       NaN  \n",
       "77     NaN       NaN       NaN       NaN       NaN  \n",
       "78     NaN       NaN       NaN       NaN       NaN  \n",
       "79     NaN       NaN       NaN       NaN       NaN  \n",
       "80     NaN       NaN       NaN       NaN       NaN  \n",
       "81     NaN       NaN       NaN       NaN       NaN  \n",
       "82     NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[83 rows x 23 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "#data.xlsxの患者番号14の右は前処理の便宜上、自分で入力したもの。\n",
    "#本当は右か左か未記入の欠損値であることに注意\n",
    "################\n",
    "\n",
    "###############\n",
    "#とりあえず一番シンプルに行こう。s.apply(lambda x: pd.Series([x, x * 2], index=['col1', 'col2']))\n",
    "#欠損データのない治療時期Aのみでモデルを作ってみる。\n",
    "###############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../dataset/data.xlsx' , delim_whitespace=True, skiprows=[1,25,42,82] )\n",
    "del data['左右'] # 左右が記入指定ある列の削除（左右に依存しないはず）\n",
    "del data['患者番号'] # 欠損データがあるため削除→インデックスを代わりに使用\n",
    "del data['視']# よくわからんが手で消せないから\n",
    "data.reset_index(drop=True)\n",
    "\n",
    "#data[[3:6],[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損データがあるので治療時期B,C,Dについては削除\n",
    "drop_col= [\"治療後Log.3\",\"治療後Log.3\",\"治療前Log.3\", \"治療前視力.3\" , \"注射時期.3\",\"治療後視力.2\",\"治療後Log.2\",\"治療後Log.2\",\"治療前Log.2\", \"治療前視力.2\" , \"注射時期.2\",\"治療後視力.1\",\"治療後Log.1\",\n",
    "           \"治療後Log.1\",\"治療前Log.1\", \"治療前視力.1\" , \"注射時期.1\",\"治療後視力.1\",\"注射時期.2\",\"治療後視力.3\",\"年/月/日.2\",\"年/月/日.1\", \"注射時期\",\"年/月/日\",\"治療後視力\",\"治療前視力\"]\n",
    "data = data.drop(drop_col , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x**2'] = data['治療前Log']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x**3'] = data['治療前Log']**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data= data.loc[:,['治療前Log','x**2',\"x**3\"]]\n",
    "y_data = data.loc[:,[\"治療後Log\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データフレームからnumpyに変換\n",
    "#x_train = x_train.as_matrix()\n",
    "#x_test = x_test.as_matrix()\n",
    "#y_train = y_train.as_matrix()\n",
    "#y_test = y_test.as_matrix()\n",
    "x_data = x_data.as_matrix()\n",
    "y_data = y_data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主成分分析する\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "x_data_pca = pca.fit_transform(x_data)\n",
    "x_data_all = np.concatenate([x_data, x_data_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ms = MinMaxScaler()\n",
    "#x_data_norm= ms.fit_transform(x_data)\n",
    "x_data_norm= ms.fit_transform(x_data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形回帰　##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一次式における決定係数は、「0.566 」\n",
      "二次式における決定係数は、「0.654 」\n",
      "三次式における決定係数は、「-205.837 」\n",
      "四次式における決定係数は、「-120961.733 」\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data_norm, y_data, test_size = 0.1, random_state = 1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_1dm = LinearRegression()\n",
    "lin_2dm = LinearRegression()\n",
    "lin_3dm = LinearRegression()\n",
    "lin_4dm = LinearRegression()\n",
    "\n",
    "degree_2 = PolynomialFeatures(degree = 2)\n",
    "degree_3 = PolynomialFeatures(degree = 3)\n",
    "degree_4 = PolynomialFeatures(degree = 4)\n",
    "\n",
    "x_train_2 = degree_2.fit_transform(x_train)\n",
    "x_train_3 = degree_3.fit_transform(x_train)\n",
    "x_train_4 = degree_4.fit_transform(x_train)\n",
    "\n",
    "lin_1dm.fit(x_train, y_train)\n",
    "lin_2dm.fit(x_train_2, y_train)\n",
    "lin_3dm.fit(x_train_3, y_train)\n",
    "lin_4dm.fit(x_train_4, y_train)\n",
    "\n",
    "x_test_2 = degree_2.fit_transform(x_test)\n",
    "x_test_3 = degree_3.fit_transform(x_test)\n",
    "x_test_4 = degree_4.fit_transform(x_test)\n",
    "\n",
    "score_1dm = lin_1dm.score(x_test, y_test)\n",
    "score_2dm = lin_2dm.score(x_test_2, y_test)\n",
    "score_3dm = lin_3dm.score(x_test_3, y_test)\n",
    "score_4dm = lin_4dm.score(x_test_4, y_test)\n",
    "\n",
    "# 決定係数は小数点以下３桁にしてみました。\n",
    "print(\"一次式における決定係数は、「%.3f\"%(score_1dm), \"」\")\n",
    "print(\"二次式における決定係数は、「%.3f\"%(score_2dm), \"」\")\n",
    "print(\"三次式における決定係数は、「%.3f\"%(score_3dm), \"」\")\n",
    "print(\"四次式における決定係数は、「%.3f\"%(score_4dm), \"」\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras の準備（ロジスティック回帰）\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "\n",
    "# create the logistic regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(5, )))\n",
    "model.add(Activation('sigmoid'))\n",
    "adam = optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shun/anaconda3/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59 samples, validate on 15 samples\n",
      "Epoch 1/500\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.6152 - acc: 0.3390 - val_loss: 0.5721 - val_acc: 0.4000\n",
      "Epoch 2/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.6144 - acc: 0.3390 - val_loss: 0.5710 - val_acc: 0.4000\n",
      "Epoch 3/500\n",
      "59/59 [==============================] - 0s 65us/step - loss: 0.6137 - acc: 0.3390 - val_loss: 0.5699 - val_acc: 0.4000\n",
      "Epoch 4/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.6130 - acc: 0.3390 - val_loss: 0.5689 - val_acc: 0.4000\n",
      "Epoch 5/500\n",
      "59/59 [==============================] - 0s 137us/step - loss: 0.6123 - acc: 0.3390 - val_loss: 0.5679 - val_acc: 0.4000\n",
      "Epoch 6/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.6115 - acc: 0.3390 - val_loss: 0.5668 - val_acc: 0.4000\n",
      "Epoch 7/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.6108 - acc: 0.3390 - val_loss: 0.5657 - val_acc: 0.4000\n",
      "Epoch 8/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.6101 - acc: 0.3390 - val_loss: 0.5647 - val_acc: 0.4000\n",
      "Epoch 9/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.6094 - acc: 0.3390 - val_loss: 0.5636 - val_acc: 0.4000\n",
      "Epoch 10/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.6086 - acc: 0.3390 - val_loss: 0.5626 - val_acc: 0.4000\n",
      "Epoch 11/500\n",
      "59/59 [==============================] - 0s 120us/step - loss: 0.6079 - acc: 0.3390 - val_loss: 0.5615 - val_acc: 0.4000\n",
      "Epoch 12/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.6072 - acc: 0.3390 - val_loss: 0.5604 - val_acc: 0.4000\n",
      "Epoch 13/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.6065 - acc: 0.3390 - val_loss: 0.5594 - val_acc: 0.4000\n",
      "Epoch 14/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.6058 - acc: 0.3390 - val_loss: 0.5584 - val_acc: 0.4000\n",
      "Epoch 15/500\n",
      "59/59 [==============================] - 0s 75us/step - loss: 0.6051 - acc: 0.3390 - val_loss: 0.5575 - val_acc: 0.4000\n",
      "Epoch 16/500\n",
      "59/59 [==============================] - 0s 154us/step - loss: 0.6045 - acc: 0.3390 - val_loss: 0.5566 - val_acc: 0.4000\n",
      "Epoch 17/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.6038 - acc: 0.3390 - val_loss: 0.5557 - val_acc: 0.4000\n",
      "Epoch 18/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.6032 - acc: 0.3390 - val_loss: 0.5548 - val_acc: 0.4000\n",
      "Epoch 19/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.6025 - acc: 0.3390 - val_loss: 0.5539 - val_acc: 0.4000\n",
      "Epoch 20/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.6019 - acc: 0.3390 - val_loss: 0.5529 - val_acc: 0.4000\n",
      "Epoch 21/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.6012 - acc: 0.3390 - val_loss: 0.5520 - val_acc: 0.4000\n",
      "Epoch 22/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.6006 - acc: 0.3390 - val_loss: 0.5511 - val_acc: 0.4000\n",
      "Epoch 23/500\n",
      "59/59 [==============================] - 0s 71us/step - loss: 0.6000 - acc: 0.3390 - val_loss: 0.5502 - val_acc: 0.4000\n",
      "Epoch 24/500\n",
      "59/59 [==============================] - 0s 136us/step - loss: 0.5993 - acc: 0.3390 - val_loss: 0.5494 - val_acc: 0.4000\n",
      "Epoch 25/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.5987 - acc: 0.3390 - val_loss: 0.5485 - val_acc: 0.4000\n",
      "Epoch 26/500\n",
      "59/59 [==============================] - 0s 110us/step - loss: 0.5981 - acc: 0.3390 - val_loss: 0.5476 - val_acc: 0.4000\n",
      "Epoch 27/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5975 - acc: 0.3390 - val_loss: 0.5467 - val_acc: 0.4000\n",
      "Epoch 28/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.5969 - acc: 0.3390 - val_loss: 0.5459 - val_acc: 0.4000\n",
      "Epoch 29/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5963 - acc: 0.3390 - val_loss: 0.5450 - val_acc: 0.4000\n",
      "Epoch 30/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5957 - acc: 0.3390 - val_loss: 0.5442 - val_acc: 0.4000\n",
      "Epoch 31/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5951 - acc: 0.3390 - val_loss: 0.5434 - val_acc: 0.4000\n",
      "Epoch 32/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5945 - acc: 0.3390 - val_loss: 0.5426 - val_acc: 0.4000\n",
      "Epoch 33/500\n",
      "59/59 [==============================] - 0s 134us/step - loss: 0.5939 - acc: 0.3390 - val_loss: 0.5417 - val_acc: 0.4000\n",
      "Epoch 34/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5934 - acc: 0.3390 - val_loss: 0.5409 - val_acc: 0.4000\n",
      "Epoch 35/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5928 - acc: 0.3390 - val_loss: 0.5401 - val_acc: 0.4000\n",
      "Epoch 36/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.5922 - acc: 0.3390 - val_loss: 0.5392 - val_acc: 0.4000\n",
      "Epoch 37/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5916 - acc: 0.3390 - val_loss: 0.5383 - val_acc: 0.4000\n",
      "Epoch 38/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5910 - acc: 0.3390 - val_loss: 0.5375 - val_acc: 0.4000\n",
      "Epoch 39/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5904 - acc: 0.3390 - val_loss: 0.5367 - val_acc: 0.4000\n",
      "Epoch 40/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5898 - acc: 0.3390 - val_loss: 0.5358 - val_acc: 0.4000\n",
      "Epoch 41/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5892 - acc: 0.3390 - val_loss: 0.5350 - val_acc: 0.4000\n",
      "Epoch 42/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.5886 - acc: 0.3390 - val_loss: 0.5342 - val_acc: 0.4000\n",
      "Epoch 43/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.5880 - acc: 0.3390 - val_loss: 0.5334 - val_acc: 0.4000\n",
      "Epoch 44/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5874 - acc: 0.3390 - val_loss: 0.5326 - val_acc: 0.4000\n",
      "Epoch 45/500\n",
      "59/59 [==============================] - 0s 134us/step - loss: 0.5869 - acc: 0.3390 - val_loss: 0.5318 - val_acc: 0.4000\n",
      "Epoch 46/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5863 - acc: 0.3390 - val_loss: 0.5309 - val_acc: 0.4000\n",
      "Epoch 47/500\n",
      "59/59 [==============================] - 0s 116us/step - loss: 0.5857 - acc: 0.3390 - val_loss: 0.5301 - val_acc: 0.4000\n",
      "Epoch 48/500\n",
      "59/59 [==============================] - 0s 122us/step - loss: 0.5851 - acc: 0.3390 - val_loss: 0.5293 - val_acc: 0.4000\n",
      "Epoch 49/500\n",
      "59/59 [==============================] - 0s 122us/step - loss: 0.5845 - acc: 0.3390 - val_loss: 0.5284 - val_acc: 0.4000\n",
      "Epoch 50/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5839 - acc: 0.3390 - val_loss: 0.5275 - val_acc: 0.4000\n",
      "Epoch 51/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.5833 - acc: 0.3390 - val_loss: 0.5266 - val_acc: 0.4000\n",
      "Epoch 52/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5827 - acc: 0.3390 - val_loss: 0.5258 - val_acc: 0.4000\n",
      "Epoch 53/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5822 - acc: 0.3390 - val_loss: 0.5249 - val_acc: 0.4000\n",
      "Epoch 54/500\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.5816 - acc: 0.3390 - val_loss: 0.5239 - val_acc: 0.4000\n",
      "Epoch 55/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5809 - acc: 0.3390 - val_loss: 0.5230 - val_acc: 0.4000\n",
      "Epoch 56/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.5804 - acc: 0.3390 - val_loss: 0.5221 - val_acc: 0.4000\n",
      "Epoch 57/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5798 - acc: 0.3390 - val_loss: 0.5212 - val_acc: 0.4000\n",
      "Epoch 58/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.5791 - acc: 0.3390 - val_loss: 0.5204 - val_acc: 0.4000\n",
      "Epoch 59/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.5786 - acc: 0.3390 - val_loss: 0.5195 - val_acc: 0.4000\n",
      "Epoch 60/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.5780 - acc: 0.3390 - val_loss: 0.5186 - val_acc: 0.4000\n",
      "Epoch 61/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.5774 - acc: 0.3390 - val_loss: 0.5177 - val_acc: 0.4000\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 116us/step - loss: 0.5768 - acc: 0.3390 - val_loss: 0.5169 - val_acc: 0.4000\n",
      "Epoch 63/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5763 - acc: 0.3390 - val_loss: 0.5160 - val_acc: 0.4000\n",
      "Epoch 64/500\n",
      "59/59 [==============================] - 0s 134us/step - loss: 0.5757 - acc: 0.3390 - val_loss: 0.5152 - val_acc: 0.4000\n",
      "Epoch 65/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5751 - acc: 0.3390 - val_loss: 0.5144 - val_acc: 0.4000\n",
      "Epoch 66/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5746 - acc: 0.3390 - val_loss: 0.5136 - val_acc: 0.4000\n",
      "Epoch 67/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5741 - acc: 0.3390 - val_loss: 0.5128 - val_acc: 0.4000\n",
      "Epoch 68/500\n",
      "59/59 [==============================] - 0s 121us/step - loss: 0.5735 - acc: 0.3390 - val_loss: 0.5120 - val_acc: 0.4000\n",
      "Epoch 69/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5730 - acc: 0.3390 - val_loss: 0.5112 - val_acc: 0.4000\n",
      "Epoch 70/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.5724 - acc: 0.3390 - val_loss: 0.5104 - val_acc: 0.4000\n",
      "Epoch 71/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.5719 - acc: 0.3390 - val_loss: 0.5097 - val_acc: 0.4000\n",
      "Epoch 72/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.5714 - acc: 0.3390 - val_loss: 0.5091 - val_acc: 0.4000\n",
      "Epoch 73/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.5709 - acc: 0.3390 - val_loss: 0.5085 - val_acc: 0.4000\n",
      "Epoch 74/500\n",
      "59/59 [==============================] - 0s 110us/step - loss: 0.5705 - acc: 0.3390 - val_loss: 0.5078 - val_acc: 0.4000\n",
      "Epoch 75/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.5700 - acc: 0.3390 - val_loss: 0.5072 - val_acc: 0.4000\n",
      "Epoch 76/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.5695 - acc: 0.3390 - val_loss: 0.5066 - val_acc: 0.4000\n",
      "Epoch 77/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5690 - acc: 0.3390 - val_loss: 0.5060 - val_acc: 0.4000\n",
      "Epoch 78/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5685 - acc: 0.3390 - val_loss: 0.5054 - val_acc: 0.4000\n",
      "Epoch 79/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5681 - acc: 0.3390 - val_loss: 0.5047 - val_acc: 0.4000\n",
      "Epoch 80/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5676 - acc: 0.3390 - val_loss: 0.5040 - val_acc: 0.4000\n",
      "Epoch 81/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5671 - acc: 0.3390 - val_loss: 0.5034 - val_acc: 0.4000\n",
      "Epoch 82/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5666 - acc: 0.3390 - val_loss: 0.5027 - val_acc: 0.4000\n",
      "Epoch 83/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.5661 - acc: 0.3390 - val_loss: 0.5020 - val_acc: 0.4000\n",
      "Epoch 84/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5656 - acc: 0.3390 - val_loss: 0.5013 - val_acc: 0.4000\n",
      "Epoch 85/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5651 - acc: 0.3390 - val_loss: 0.5005 - val_acc: 0.4000\n",
      "Epoch 86/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5646 - acc: 0.3390 - val_loss: 0.4998 - val_acc: 0.4000\n",
      "Epoch 87/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.5641 - acc: 0.3390 - val_loss: 0.4991 - val_acc: 0.4000\n",
      "Epoch 88/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5636 - acc: 0.3390 - val_loss: 0.4985 - val_acc: 0.4000\n",
      "Epoch 89/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5631 - acc: 0.3390 - val_loss: 0.4978 - val_acc: 0.4000\n",
      "Epoch 90/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5626 - acc: 0.3390 - val_loss: 0.4971 - val_acc: 0.4000\n",
      "Epoch 91/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.5622 - acc: 0.3390 - val_loss: 0.4964 - val_acc: 0.4000\n",
      "Epoch 92/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5617 - acc: 0.3390 - val_loss: 0.4957 - val_acc: 0.4000\n",
      "Epoch 93/500\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.5612 - acc: 0.3390 - val_loss: 0.4950 - val_acc: 0.4000\n",
      "Epoch 94/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.5607 - acc: 0.3390 - val_loss: 0.4944 - val_acc: 0.4000\n",
      "Epoch 95/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.5603 - acc: 0.3390 - val_loss: 0.4938 - val_acc: 0.4000\n",
      "Epoch 96/500\n",
      "59/59 [==============================] - 0s 75us/step - loss: 0.5598 - acc: 0.3390 - val_loss: 0.4932 - val_acc: 0.4000\n",
      "Epoch 97/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.5593 - acc: 0.3390 - val_loss: 0.4925 - val_acc: 0.4000\n",
      "Epoch 98/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.5588 - acc: 0.3390 - val_loss: 0.4919 - val_acc: 0.4000\n",
      "Epoch 99/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5584 - acc: 0.3390 - val_loss: 0.4912 - val_acc: 0.4000\n",
      "Epoch 100/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5579 - acc: 0.3390 - val_loss: 0.4905 - val_acc: 0.4000\n",
      "Epoch 101/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.5574 - acc: 0.3390 - val_loss: 0.4898 - val_acc: 0.4000\n",
      "Epoch 102/500\n",
      "59/59 [==============================] - 0s 159us/step - loss: 0.5569 - acc: 0.3390 - val_loss: 0.4891 - val_acc: 0.4000\n",
      "Epoch 103/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.5564 - acc: 0.3390 - val_loss: 0.4885 - val_acc: 0.4000\n",
      "Epoch 104/500\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.5560 - acc: 0.3390 - val_loss: 0.4878 - val_acc: 0.4000\n",
      "Epoch 105/500\n",
      "59/59 [==============================] - 0s 146us/step - loss: 0.5555 - acc: 0.3390 - val_loss: 0.4872 - val_acc: 0.4000\n",
      "Epoch 106/500\n",
      "59/59 [==============================] - 0s 117us/step - loss: 0.5550 - acc: 0.3390 - val_loss: 0.4866 - val_acc: 0.4000\n",
      "Epoch 107/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5546 - acc: 0.3390 - val_loss: 0.4860 - val_acc: 0.4000\n",
      "Epoch 108/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.5541 - acc: 0.3390 - val_loss: 0.4853 - val_acc: 0.4000\n",
      "Epoch 109/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.5536 - acc: 0.3390 - val_loss: 0.4846 - val_acc: 0.4000\n",
      "Epoch 110/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.5532 - acc: 0.3390 - val_loss: 0.4839 - val_acc: 0.4000\n",
      "Epoch 111/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5527 - acc: 0.3390 - val_loss: 0.4833 - val_acc: 0.4000\n",
      "Epoch 112/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.5522 - acc: 0.3390 - val_loss: 0.4826 - val_acc: 0.4000\n",
      "Epoch 113/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5518 - acc: 0.3390 - val_loss: 0.4820 - val_acc: 0.4000\n",
      "Epoch 114/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5514 - acc: 0.3390 - val_loss: 0.4814 - val_acc: 0.4000\n",
      "Epoch 115/500\n",
      "59/59 [==============================] - 0s 137us/step - loss: 0.5509 - acc: 0.3390 - val_loss: 0.4808 - val_acc: 0.4000\n",
      "Epoch 116/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.5505 - acc: 0.3390 - val_loss: 0.4802 - val_acc: 0.4000\n",
      "Epoch 117/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.5501 - acc: 0.3390 - val_loss: 0.4796 - val_acc: 0.4000\n",
      "Epoch 118/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.5496 - acc: 0.3390 - val_loss: 0.4790 - val_acc: 0.4000\n",
      "Epoch 119/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.5492 - acc: 0.3390 - val_loss: 0.4785 - val_acc: 0.4000\n",
      "Epoch 120/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5488 - acc: 0.3390 - val_loss: 0.4779 - val_acc: 0.4000\n",
      "Epoch 121/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5484 - acc: 0.3390 - val_loss: 0.4774 - val_acc: 0.4000\n",
      "Epoch 122/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.5479 - acc: 0.3390 - val_loss: 0.4769 - val_acc: 0.4000\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 111us/step - loss: 0.5475 - acc: 0.3390 - val_loss: 0.4764 - val_acc: 0.4000\n",
      "Epoch 124/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5471 - acc: 0.3390 - val_loss: 0.4759 - val_acc: 0.4000\n",
      "Epoch 125/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5467 - acc: 0.3390 - val_loss: 0.4753 - val_acc: 0.4000\n",
      "Epoch 126/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.5462 - acc: 0.3390 - val_loss: 0.4747 - val_acc: 0.4000\n",
      "Epoch 127/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.5458 - acc: 0.3390 - val_loss: 0.4742 - val_acc: 0.4000\n",
      "Epoch 128/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.5454 - acc: 0.3390 - val_loss: 0.4736 - val_acc: 0.4000\n",
      "Epoch 129/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5449 - acc: 0.3390 - val_loss: 0.4730 - val_acc: 0.4000\n",
      "Epoch 130/500\n",
      "59/59 [==============================] - 0s 140us/step - loss: 0.5445 - acc: 0.3390 - val_loss: 0.4725 - val_acc: 0.4000\n",
      "Epoch 131/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5548 - acc: 0.280 - 0s 88us/step - loss: 0.5441 - acc: 0.3390 - val_loss: 0.4720 - val_acc: 0.4000\n",
      "Epoch 132/500\n",
      "59/59 [==============================] - 0s 117us/step - loss: 0.5436 - acc: 0.3390 - val_loss: 0.4714 - val_acc: 0.4000\n",
      "Epoch 133/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5432 - acc: 0.3390 - val_loss: 0.4708 - val_acc: 0.4000\n",
      "Epoch 134/500\n",
      "59/59 [==============================] - 0s 121us/step - loss: 0.5428 - acc: 0.3390 - val_loss: 0.4702 - val_acc: 0.4000\n",
      "Epoch 135/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.5423 - acc: 0.3390 - val_loss: 0.4697 - val_acc: 0.4000\n",
      "Epoch 136/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.5419 - acc: 0.3390 - val_loss: 0.4691 - val_acc: 0.4000\n",
      "Epoch 137/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5415 - acc: 0.3390 - val_loss: 0.4686 - val_acc: 0.4000\n",
      "Epoch 138/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5411 - acc: 0.3390 - val_loss: 0.4680 - val_acc: 0.4000\n",
      "Epoch 139/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5407 - acc: 0.3390 - val_loss: 0.4674 - val_acc: 0.4000\n",
      "Epoch 140/500\n",
      "59/59 [==============================] - 0s 110us/step - loss: 0.5403 - acc: 0.3390 - val_loss: 0.4669 - val_acc: 0.4000\n",
      "Epoch 141/500\n",
      "59/59 [==============================] - 0s 127us/step - loss: 0.5398 - acc: 0.3390 - val_loss: 0.4663 - val_acc: 0.4000\n",
      "Epoch 142/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5439 - acc: 0.360 - 0s 127us/step - loss: 0.5394 - acc: 0.3390 - val_loss: 0.4657 - val_acc: 0.4000\n",
      "Epoch 143/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5390 - acc: 0.3390 - val_loss: 0.4651 - val_acc: 0.4000\n",
      "Epoch 144/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.5386 - acc: 0.3390 - val_loss: 0.4645 - val_acc: 0.4000\n",
      "Epoch 145/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5382 - acc: 0.3390 - val_loss: 0.4639 - val_acc: 0.4000\n",
      "Epoch 146/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.5378 - acc: 0.3390 - val_loss: 0.4634 - val_acc: 0.4000\n",
      "Epoch 147/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.5374 - acc: 0.3390 - val_loss: 0.4628 - val_acc: 0.4000\n",
      "Epoch 148/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5370 - acc: 0.3390 - val_loss: 0.4623 - val_acc: 0.4000\n",
      "Epoch 149/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5366 - acc: 0.3390 - val_loss: 0.4618 - val_acc: 0.4000\n",
      "Epoch 150/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.5362 - acc: 0.3390 - val_loss: 0.4613 - val_acc: 0.4000\n",
      "Epoch 151/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5358 - acc: 0.3390 - val_loss: 0.4608 - val_acc: 0.4000\n",
      "Epoch 152/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.5354 - acc: 0.3390 - val_loss: 0.4603 - val_acc: 0.4000\n",
      "Epoch 153/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.5350 - acc: 0.3390 - val_loss: 0.4597 - val_acc: 0.4000\n",
      "Epoch 154/500\n",
      "59/59 [==============================] - 0s 148us/step - loss: 0.5346 - acc: 0.3390 - val_loss: 0.4592 - val_acc: 0.4000\n",
      "Epoch 155/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5342 - acc: 0.3390 - val_loss: 0.4586 - val_acc: 0.4000\n",
      "Epoch 156/500\n",
      "59/59 [==============================] - 0s 116us/step - loss: 0.5338 - acc: 0.3390 - val_loss: 0.4580 - val_acc: 0.4000\n",
      "Epoch 157/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5334 - acc: 0.3390 - val_loss: 0.4574 - val_acc: 0.4000\n",
      "Epoch 158/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5330 - acc: 0.3390 - val_loss: 0.4568 - val_acc: 0.4000\n",
      "Epoch 159/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5326 - acc: 0.3390 - val_loss: 0.4562 - val_acc: 0.4000\n",
      "Epoch 160/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5322 - acc: 0.3390 - val_loss: 0.4557 - val_acc: 0.4000\n",
      "Epoch 161/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5319 - acc: 0.3390 - val_loss: 0.4552 - val_acc: 0.4000\n",
      "Epoch 162/500\n",
      "59/59 [==============================] - 0s 133us/step - loss: 0.5315 - acc: 0.3390 - val_loss: 0.4547 - val_acc: 0.4000\n",
      "Epoch 163/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5311 - acc: 0.3390 - val_loss: 0.4542 - val_acc: 0.4000\n",
      "Epoch 164/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.5307 - acc: 0.3390 - val_loss: 0.4537 - val_acc: 0.4000\n",
      "Epoch 165/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.5303 - acc: 0.3390 - val_loss: 0.4532 - val_acc: 0.4000\n",
      "Epoch 166/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.5299 - acc: 0.3390 - val_loss: 0.4527 - val_acc: 0.4000\n",
      "Epoch 167/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5296 - acc: 0.3390 - val_loss: 0.4522 - val_acc: 0.4000\n",
      "Epoch 168/500\n",
      "59/59 [==============================] - 0s 146us/step - loss: 0.5292 - acc: 0.3390 - val_loss: 0.4516 - val_acc: 0.4000\n",
      "Epoch 169/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.5288 - acc: 0.3390 - val_loss: 0.4511 - val_acc: 0.4000\n",
      "Epoch 170/500\n",
      "59/59 [==============================] - 0s 121us/step - loss: 0.5284 - acc: 0.3390 - val_loss: 0.4505 - val_acc: 0.4000\n",
      "Epoch 171/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.5280 - acc: 0.3390 - val_loss: 0.4500 - val_acc: 0.4000\n",
      "Epoch 172/500\n",
      "59/59 [==============================] - 0s 143us/step - loss: 0.5277 - acc: 0.3390 - val_loss: 0.4496 - val_acc: 0.4000\n",
      "Epoch 173/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5273 - acc: 0.3390 - val_loss: 0.4491 - val_acc: 0.4000\n",
      "Epoch 174/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.5269 - acc: 0.3390 - val_loss: 0.4487 - val_acc: 0.4000\n",
      "Epoch 175/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.5266 - acc: 0.3390 - val_loss: 0.4483 - val_acc: 0.4000\n",
      "Epoch 176/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5262 - acc: 0.3390 - val_loss: 0.4479 - val_acc: 0.4000\n",
      "Epoch 177/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5258 - acc: 0.3390 - val_loss: 0.4476 - val_acc: 0.4000\n",
      "Epoch 178/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5255 - acc: 0.3390 - val_loss: 0.4472 - val_acc: 0.4000\n",
      "Epoch 179/500\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.5252 - acc: 0.3390 - val_loss: 0.4468 - val_acc: 0.4000\n",
      "Epoch 180/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5344 - acc: 0.300 - 0s 127us/step - loss: 0.5248 - acc: 0.3390 - val_loss: 0.4464 - val_acc: 0.4000\n",
      "Epoch 181/500\n",
      "59/59 [==============================] - 0s 73us/step - loss: 0.5245 - acc: 0.3390 - val_loss: 0.4459 - val_acc: 0.4000\n",
      "Epoch 182/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5304 - acc: 0.360 - 0s 106us/step - loss: 0.5241 - acc: 0.3390 - val_loss: 0.4454 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5238 - acc: 0.3390 - val_loss: 0.4449 - val_acc: 0.4000\n",
      "Epoch 184/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.5234 - acc: 0.3390 - val_loss: 0.4445 - val_acc: 0.4000\n",
      "Epoch 185/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.5231 - acc: 0.3390 - val_loss: 0.4441 - val_acc: 0.4000\n",
      "Epoch 186/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.5228 - acc: 0.3390 - val_loss: 0.4437 - val_acc: 0.4000\n",
      "Epoch 187/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5224 - acc: 0.3390 - val_loss: 0.4432 - val_acc: 0.4000\n",
      "Epoch 188/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5221 - acc: 0.3390 - val_loss: 0.4427 - val_acc: 0.4000\n",
      "Epoch 189/500\n",
      "59/59 [==============================] - 0s 117us/step - loss: 0.5217 - acc: 0.3390 - val_loss: 0.4422 - val_acc: 0.4000\n",
      "Epoch 190/500\n",
      "59/59 [==============================] - 0s 167us/step - loss: 0.5214 - acc: 0.3390 - val_loss: 0.4417 - val_acc: 0.4000\n",
      "Epoch 191/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5210 - acc: 0.3390 - val_loss: 0.4413 - val_acc: 0.4000\n",
      "Epoch 192/500\n",
      "59/59 [==============================] - 0s 135us/step - loss: 0.5207 - acc: 0.3390 - val_loss: 0.4408 - val_acc: 0.4000\n",
      "Epoch 193/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.5203 - acc: 0.3390 - val_loss: 0.4404 - val_acc: 0.4000\n",
      "Epoch 194/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5200 - acc: 0.3390 - val_loss: 0.4399 - val_acc: 0.4000\n",
      "Epoch 195/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5197 - acc: 0.3390 - val_loss: 0.4395 - val_acc: 0.4000\n",
      "Epoch 196/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5193 - acc: 0.3390 - val_loss: 0.4390 - val_acc: 0.4000\n",
      "Epoch 197/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.5190 - acc: 0.3390 - val_loss: 0.4386 - val_acc: 0.4000\n",
      "Epoch 198/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5305 - acc: 0.340 - 0s 104us/step - loss: 0.5187 - acc: 0.3390 - val_loss: 0.4383 - val_acc: 0.4000\n",
      "Epoch 199/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.5184 - acc: 0.3390 - val_loss: 0.4378 - val_acc: 0.4000\n",
      "Epoch 200/500\n",
      "59/59 [==============================] - 0s 129us/step - loss: 0.5180 - acc: 0.3390 - val_loss: 0.4374 - val_acc: 0.4000\n",
      "Epoch 201/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5177 - acc: 0.3390 - val_loss: 0.4370 - val_acc: 0.4000\n",
      "Epoch 202/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5173 - acc: 0.3390 - val_loss: 0.4365 - val_acc: 0.4000\n",
      "Epoch 203/500\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.5170 - acc: 0.3390 - val_loss: 0.4360 - val_acc: 0.4000\n",
      "Epoch 204/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5167 - acc: 0.3390 - val_loss: 0.4356 - val_acc: 0.4000\n",
      "Epoch 205/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.5163 - acc: 0.3390 - val_loss: 0.4352 - val_acc: 0.4000\n",
      "Epoch 206/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.5160 - acc: 0.3390 - val_loss: 0.4347 - val_acc: 0.4000\n",
      "Epoch 207/500\n",
      "59/59 [==============================] - 0s 120us/step - loss: 0.5156 - acc: 0.3390 - val_loss: 0.4344 - val_acc: 0.4000\n",
      "Epoch 208/500\n",
      "59/59 [==============================] - 0s 164us/step - loss: 0.5153 - acc: 0.3390 - val_loss: 0.4340 - val_acc: 0.4000\n",
      "Epoch 209/500\n",
      "59/59 [==============================] - 0s 121us/step - loss: 0.5150 - acc: 0.3390 - val_loss: 0.4336 - val_acc: 0.4000\n",
      "Epoch 210/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.5147 - acc: 0.3390 - val_loss: 0.4332 - val_acc: 0.4000\n",
      "Epoch 211/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.5143 - acc: 0.3390 - val_loss: 0.4328 - val_acc: 0.4000\n",
      "Epoch 212/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.5140 - acc: 0.3390 - val_loss: 0.4323 - val_acc: 0.4000\n",
      "Epoch 213/500\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.5137 - acc: 0.3390 - val_loss: 0.4319 - val_acc: 0.4000\n",
      "Epoch 214/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5133 - acc: 0.3390 - val_loss: 0.4315 - val_acc: 0.4000\n",
      "Epoch 215/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.5130 - acc: 0.3390 - val_loss: 0.4311 - val_acc: 0.4000\n",
      "Epoch 216/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.5127 - acc: 0.3390 - val_loss: 0.4308 - val_acc: 0.4000\n",
      "Epoch 217/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5124 - acc: 0.3390 - val_loss: 0.4304 - val_acc: 0.4000\n",
      "Epoch 218/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5121 - acc: 0.3390 - val_loss: 0.4300 - val_acc: 0.4000\n",
      "Epoch 219/500\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5118 - acc: 0.3390 - val_loss: 0.4295 - val_acc: 0.4000\n",
      "Epoch 220/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5115 - acc: 0.3390 - val_loss: 0.4291 - val_acc: 0.4000\n",
      "Epoch 221/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.5112 - acc: 0.3390 - val_loss: 0.4287 - val_acc: 0.4000\n",
      "Epoch 222/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.5108 - acc: 0.3390 - val_loss: 0.4284 - val_acc: 0.4000\n",
      "Epoch 223/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.5105 - acc: 0.3390 - val_loss: 0.4281 - val_acc: 0.4000\n",
      "Epoch 224/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.5102 - acc: 0.3390 - val_loss: 0.4278 - val_acc: 0.4000\n",
      "Epoch 225/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.5103 - acc: 0.340 - 0s 87us/step - loss: 0.5099 - acc: 0.3390 - val_loss: 0.4275 - val_acc: 0.4000\n",
      "Epoch 226/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.5096 - acc: 0.3390 - val_loss: 0.4271 - val_acc: 0.4000\n",
      "Epoch 227/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.5093 - acc: 0.3390 - val_loss: 0.4267 - val_acc: 0.4000\n",
      "Epoch 228/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4970 - acc: 0.340 - 0s 81us/step - loss: 0.5090 - acc: 0.3390 - val_loss: 0.4264 - val_acc: 0.4000\n",
      "Epoch 229/500\n",
      "59/59 [==============================] - 0s 124us/step - loss: 0.5087 - acc: 0.3390 - val_loss: 0.4260 - val_acc: 0.4000\n",
      "Epoch 230/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.5084 - acc: 0.3390 - val_loss: 0.4257 - val_acc: 0.4000\n",
      "Epoch 231/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.5081 - acc: 0.3390 - val_loss: 0.4253 - val_acc: 0.4000\n",
      "Epoch 232/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.5077 - acc: 0.3390 - val_loss: 0.4249 - val_acc: 0.4000\n",
      "Epoch 233/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.5074 - acc: 0.3390 - val_loss: 0.4246 - val_acc: 0.4000\n",
      "Epoch 234/500\n",
      "59/59 [==============================] - 0s 77us/step - loss: 0.5071 - acc: 0.3390 - val_loss: 0.4242 - val_acc: 0.4000\n",
      "Epoch 235/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5068 - acc: 0.3390 - val_loss: 0.4238 - val_acc: 0.4000\n",
      "Epoch 236/500\n",
      "59/59 [==============================] - 0s 125us/step - loss: 0.5065 - acc: 0.3390 - val_loss: 0.4234 - val_acc: 0.4000\n",
      "Epoch 237/500\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.5062 - acc: 0.3390 - val_loss: 0.4230 - val_acc: 0.4000\n",
      "Epoch 238/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.5059 - acc: 0.3390 - val_loss: 0.4226 - val_acc: 0.4000\n",
      "Epoch 239/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.5056 - acc: 0.3390 - val_loss: 0.4222 - val_acc: 0.4000\n",
      "Epoch 240/500\n",
      "59/59 [==============================] - 0s 154us/step - loss: 0.5053 - acc: 0.3390 - val_loss: 0.4219 - val_acc: 0.4000\n",
      "Epoch 241/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.5050 - acc: 0.3390 - val_loss: 0.4215 - val_acc: 0.4000\n",
      "Epoch 242/500\n",
      "59/59 [==============================] - 0s 130us/step - loss: 0.5047 - acc: 0.3390 - val_loss: 0.4212 - val_acc: 0.4000\n",
      "Epoch 243/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 95us/step - loss: 0.5044 - acc: 0.3390 - val_loss: 0.4209 - val_acc: 0.4000\n",
      "Epoch 244/500\n",
      "59/59 [==============================] - 0s 110us/step - loss: 0.5041 - acc: 0.3390 - val_loss: 0.4205 - val_acc: 0.4000\n",
      "Epoch 245/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.5038 - acc: 0.3390 - val_loss: 0.4202 - val_acc: 0.4000\n",
      "Epoch 246/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5035 - acc: 0.3390 - val_loss: 0.4199 - val_acc: 0.4000\n",
      "Epoch 247/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.5032 - acc: 0.3390 - val_loss: 0.4196 - val_acc: 0.4000\n",
      "Epoch 248/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.5029 - acc: 0.3390 - val_loss: 0.4193 - val_acc: 0.4000\n",
      "Epoch 249/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5026 - acc: 0.3390 - val_loss: 0.4191 - val_acc: 0.4000\n",
      "Epoch 250/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5023 - acc: 0.3390 - val_loss: 0.4188 - val_acc: 0.4000\n",
      "Epoch 251/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.5021 - acc: 0.3390 - val_loss: 0.4185 - val_acc: 0.4000\n",
      "Epoch 252/500\n",
      "59/59 [==============================] - 0s 191us/step - loss: 0.5018 - acc: 0.3390 - val_loss: 0.4182 - val_acc: 0.4000\n",
      "Epoch 253/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.5015 - acc: 0.3390 - val_loss: 0.4178 - val_acc: 0.4000\n",
      "Epoch 254/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.5012 - acc: 0.3390 - val_loss: 0.4174 - val_acc: 0.4000\n",
      "Epoch 255/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.5009 - acc: 0.3390 - val_loss: 0.4171 - val_acc: 0.4000\n",
      "Epoch 256/500\n",
      "59/59 [==============================] - 0s 122us/step - loss: 0.5006 - acc: 0.3390 - val_loss: 0.4167 - val_acc: 0.4000\n",
      "Epoch 257/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.5003 - acc: 0.3390 - val_loss: 0.4164 - val_acc: 0.4000\n",
      "Epoch 258/500\n",
      "59/59 [==============================] - 0s 135us/step - loss: 0.5000 - acc: 0.3390 - val_loss: 0.4161 - val_acc: 0.4000\n",
      "Epoch 259/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.4998 - acc: 0.3390 - val_loss: 0.4158 - val_acc: 0.4000\n",
      "Epoch 260/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4995 - acc: 0.3390 - val_loss: 0.4155 - val_acc: 0.4000\n",
      "Epoch 261/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4992 - acc: 0.3390 - val_loss: 0.4152 - val_acc: 0.4000\n",
      "Epoch 262/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4989 - acc: 0.3390 - val_loss: 0.4149 - val_acc: 0.4000\n",
      "Epoch 263/500\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.4986 - acc: 0.3390 - val_loss: 0.4146 - val_acc: 0.4000\n",
      "Epoch 264/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4984 - acc: 0.3390 - val_loss: 0.4142 - val_acc: 0.4000\n",
      "Epoch 265/500\n",
      "59/59 [==============================] - 0s 116us/step - loss: 0.4981 - acc: 0.3390 - val_loss: 0.4139 - val_acc: 0.4000\n",
      "Epoch 266/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.4978 - acc: 0.3390 - val_loss: 0.4136 - val_acc: 0.4000\n",
      "Epoch 267/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.4975 - acc: 0.3390 - val_loss: 0.4132 - val_acc: 0.4000\n",
      "Epoch 268/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4972 - acc: 0.3390 - val_loss: 0.4128 - val_acc: 0.4000\n",
      "Epoch 269/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4933 - acc: 0.360 - 0s 121us/step - loss: 0.4969 - acc: 0.3390 - val_loss: 0.4124 - val_acc: 0.4000\n",
      "Epoch 270/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4966 - acc: 0.3390 - val_loss: 0.4120 - val_acc: 0.4000\n",
      "Epoch 271/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.4963 - acc: 0.3390 - val_loss: 0.4116 - val_acc: 0.4000\n",
      "Epoch 272/500\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.4961 - acc: 0.3390 - val_loss: 0.4112 - val_acc: 0.4000\n",
      "Epoch 273/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4958 - acc: 0.3390 - val_loss: 0.4108 - val_acc: 0.4000\n",
      "Epoch 274/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4955 - acc: 0.3390 - val_loss: 0.4105 - val_acc: 0.4000\n",
      "Epoch 275/500\n",
      "59/59 [==============================] - 0s 164us/step - loss: 0.4952 - acc: 0.3390 - val_loss: 0.4101 - val_acc: 0.4000\n",
      "Epoch 276/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4949 - acc: 0.3390 - val_loss: 0.4098 - val_acc: 0.4000\n",
      "Epoch 277/500\n",
      "59/59 [==============================] - 0s 158us/step - loss: 0.4947 - acc: 0.3390 - val_loss: 0.4096 - val_acc: 0.4000\n",
      "Epoch 278/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4944 - acc: 0.3390 - val_loss: 0.4093 - val_acc: 0.4000\n",
      "Epoch 279/500\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.4941 - acc: 0.3390 - val_loss: 0.4090 - val_acc: 0.4000\n",
      "Epoch 280/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4939 - acc: 0.3390 - val_loss: 0.4087 - val_acc: 0.4000\n",
      "Epoch 281/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.4936 - acc: 0.3390 - val_loss: 0.4083 - val_acc: 0.4000\n",
      "Epoch 282/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.4933 - acc: 0.3390 - val_loss: 0.4080 - val_acc: 0.4000\n",
      "Epoch 283/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4931 - acc: 0.3390 - val_loss: 0.4077 - val_acc: 0.4000\n",
      "Epoch 284/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.4928 - acc: 0.3390 - val_loss: 0.4074 - val_acc: 0.4000\n",
      "Epoch 285/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4925 - acc: 0.3390 - val_loss: 0.4072 - val_acc: 0.4000\n",
      "Epoch 286/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4923 - acc: 0.3390 - val_loss: 0.4070 - val_acc: 0.4000\n",
      "Epoch 287/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.4920 - acc: 0.3390 - val_loss: 0.4067 - val_acc: 0.4000\n",
      "Epoch 288/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.4917 - acc: 0.3390 - val_loss: 0.4065 - val_acc: 0.4000\n",
      "Epoch 289/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4915 - acc: 0.3390 - val_loss: 0.4063 - val_acc: 0.4000\n",
      "Epoch 290/500\n",
      "59/59 [==============================] - 0s 141us/step - loss: 0.4912 - acc: 0.3390 - val_loss: 0.4061 - val_acc: 0.4000\n",
      "Epoch 291/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4910 - acc: 0.3390 - val_loss: 0.4059 - val_acc: 0.4000\n",
      "Epoch 292/500\n",
      "59/59 [==============================] - 0s 121us/step - loss: 0.4907 - acc: 0.3390 - val_loss: 0.4058 - val_acc: 0.4000\n",
      "Epoch 293/500\n",
      "59/59 [==============================] - 0s 73us/step - loss: 0.4904 - acc: 0.3390 - val_loss: 0.4056 - val_acc: 0.4000\n",
      "Epoch 294/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.4902 - acc: 0.3390 - val_loss: 0.4055 - val_acc: 0.4000\n",
      "Epoch 295/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4899 - acc: 0.3390 - val_loss: 0.4054 - val_acc: 0.4000\n",
      "Epoch 296/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4897 - acc: 0.3390 - val_loss: 0.4052 - val_acc: 0.4000\n",
      "Epoch 297/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.4894 - acc: 0.3390 - val_loss: 0.4050 - val_acc: 0.4000\n",
      "Epoch 298/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4892 - acc: 0.3390 - val_loss: 0.4047 - val_acc: 0.4000\n",
      "Epoch 299/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.4889 - acc: 0.3390 - val_loss: 0.4045 - val_acc: 0.4000\n",
      "Epoch 300/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.4887 - acc: 0.3390 - val_loss: 0.4043 - val_acc: 0.4000\n",
      "Epoch 301/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.4884 - acc: 0.3390 - val_loss: 0.4042 - val_acc: 0.4000\n",
      "Epoch 302/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.4882 - acc: 0.3390 - val_loss: 0.4039 - val_acc: 0.4000\n",
      "Epoch 303/500\n",
      "59/59 [==============================] - 0s 120us/step - loss: 0.4879 - acc: 0.3390 - val_loss: 0.4037 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.4877 - acc: 0.3390 - val_loss: 0.4034 - val_acc: 0.4000\n",
      "Epoch 305/500\n",
      "59/59 [==============================] - 0s 150us/step - loss: 0.4874 - acc: 0.3390 - val_loss: 0.4031 - val_acc: 0.4000\n",
      "Epoch 306/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4872 - acc: 0.3390 - val_loss: 0.4028 - val_acc: 0.4000\n",
      "Epoch 307/500\n",
      "59/59 [==============================] - 0s 146us/step - loss: 0.4869 - acc: 0.3390 - val_loss: 0.4025 - val_acc: 0.4000\n",
      "Epoch 308/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4867 - acc: 0.3390 - val_loss: 0.4022 - val_acc: 0.4000\n",
      "Epoch 309/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4864 - acc: 0.3390 - val_loss: 0.4019 - val_acc: 0.4000\n",
      "Epoch 310/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4861 - acc: 0.3390 - val_loss: 0.4015 - val_acc: 0.4000\n",
      "Epoch 311/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4859 - acc: 0.3390 - val_loss: 0.4012 - val_acc: 0.4000\n",
      "Epoch 312/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4985 - acc: 0.340 - 0s 80us/step - loss: 0.4856 - acc: 0.3390 - val_loss: 0.4008 - val_acc: 0.4000\n",
      "Epoch 313/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4796 - acc: 0.300 - 0s 99us/step - loss: 0.4854 - acc: 0.3390 - val_loss: 0.4005 - val_acc: 0.4000\n",
      "Epoch 314/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4851 - acc: 0.3390 - val_loss: 0.4002 - val_acc: 0.4000\n",
      "Epoch 315/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.4849 - acc: 0.3390 - val_loss: 0.3999 - val_acc: 0.4000\n",
      "Epoch 316/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4846 - acc: 0.3390 - val_loss: 0.3996 - val_acc: 0.4000\n",
      "Epoch 317/500\n",
      "59/59 [==============================] - 0s 125us/step - loss: 0.4843 - acc: 0.3390 - val_loss: 0.3993 - val_acc: 0.4000\n",
      "Epoch 318/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.4841 - acc: 0.3390 - val_loss: 0.3989 - val_acc: 0.4000\n",
      "Epoch 319/500\n",
      "59/59 [==============================] - 0s 128us/step - loss: 0.4838 - acc: 0.3390 - val_loss: 0.3985 - val_acc: 0.4000\n",
      "Epoch 320/500\n",
      "59/59 [==============================] - 0s 122us/step - loss: 0.4836 - acc: 0.3390 - val_loss: 0.3982 - val_acc: 0.4000\n",
      "Epoch 321/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4833 - acc: 0.3390 - val_loss: 0.3979 - val_acc: 0.4000\n",
      "Epoch 322/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.4831 - acc: 0.3390 - val_loss: 0.3977 - val_acc: 0.4000\n",
      "Epoch 323/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4828 - acc: 0.3390 - val_loss: 0.3974 - val_acc: 0.4000\n",
      "Epoch 324/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4826 - acc: 0.3390 - val_loss: 0.3971 - val_acc: 0.4000\n",
      "Epoch 325/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.4823 - acc: 0.3390 - val_loss: 0.3968 - val_acc: 0.4000\n",
      "Epoch 326/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4821 - acc: 0.3390 - val_loss: 0.3965 - val_acc: 0.4000\n",
      "Epoch 327/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4819 - acc: 0.3390 - val_loss: 0.3962 - val_acc: 0.4000\n",
      "Epoch 328/500\n",
      "59/59 [==============================] - 0s 152us/step - loss: 0.4816 - acc: 0.3390 - val_loss: 0.3959 - val_acc: 0.4000\n",
      "Epoch 329/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4814 - acc: 0.3390 - val_loss: 0.3957 - val_acc: 0.4000\n",
      "Epoch 330/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.4811 - acc: 0.3390 - val_loss: 0.3954 - val_acc: 0.4000\n",
      "Epoch 331/500\n",
      "59/59 [==============================] - 0s 71us/step - loss: 0.4809 - acc: 0.3390 - val_loss: 0.3952 - val_acc: 0.4000\n",
      "Epoch 332/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4807 - acc: 0.3390 - val_loss: 0.3950 - val_acc: 0.4000\n",
      "Epoch 333/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.4804 - acc: 0.3390 - val_loss: 0.3948 - val_acc: 0.4000\n",
      "Epoch 334/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4802 - acc: 0.3390 - val_loss: 0.3946 - val_acc: 0.4000\n",
      "Epoch 335/500\n",
      "59/59 [==============================] - 0s 77us/step - loss: 0.4800 - acc: 0.3390 - val_loss: 0.3944 - val_acc: 0.4000\n",
      "Epoch 336/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4797 - acc: 0.3390 - val_loss: 0.3942 - val_acc: 0.4000\n",
      "Epoch 337/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.4795 - acc: 0.3390 - val_loss: 0.3940 - val_acc: 0.4000\n",
      "Epoch 338/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4793 - acc: 0.3390 - val_loss: 0.3938 - val_acc: 0.4000\n",
      "Epoch 339/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4791 - acc: 0.3390 - val_loss: 0.3936 - val_acc: 0.4000\n",
      "Epoch 340/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.4788 - acc: 0.3390 - val_loss: 0.3934 - val_acc: 0.4000\n",
      "Epoch 341/500\n",
      "59/59 [==============================] - 0s 120us/step - loss: 0.4786 - acc: 0.3390 - val_loss: 0.3932 - val_acc: 0.4000\n",
      "Epoch 342/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.4784 - acc: 0.3390 - val_loss: 0.3930 - val_acc: 0.4000\n",
      "Epoch 343/500\n",
      "59/59 [==============================] - 0s 135us/step - loss: 0.4781 - acc: 0.3390 - val_loss: 0.3927 - val_acc: 0.4000\n",
      "Epoch 344/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4779 - acc: 0.3390 - val_loss: 0.3924 - val_acc: 0.4000\n",
      "Epoch 345/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.4776 - acc: 0.3390 - val_loss: 0.3922 - val_acc: 0.4000\n",
      "Epoch 346/500\n",
      "59/59 [==============================] - 0s 70us/step - loss: 0.4774 - acc: 0.3390 - val_loss: 0.3920 - val_acc: 0.4000\n",
      "Epoch 347/500\n",
      "59/59 [==============================] - 0s 153us/step - loss: 0.4772 - acc: 0.3390 - val_loss: 0.3918 - val_acc: 0.4000\n",
      "Epoch 348/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.4769 - acc: 0.3390 - val_loss: 0.3915 - val_acc: 0.4000\n",
      "Epoch 349/500\n",
      "59/59 [==============================] - 0s 119us/step - loss: 0.4767 - acc: 0.3390 - val_loss: 0.3913 - val_acc: 0.4000\n",
      "Epoch 350/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4765 - acc: 0.3390 - val_loss: 0.3910 - val_acc: 0.4000\n",
      "Epoch 351/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4763 - acc: 0.3390 - val_loss: 0.3908 - val_acc: 0.4000\n",
      "Epoch 352/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4760 - acc: 0.3390 - val_loss: 0.3906 - val_acc: 0.4000\n",
      "Epoch 353/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.4758 - acc: 0.3390 - val_loss: 0.3905 - val_acc: 0.4000\n",
      "Epoch 354/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4756 - acc: 0.3390 - val_loss: 0.3903 - val_acc: 0.4000\n",
      "Epoch 355/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4754 - acc: 0.3390 - val_loss: 0.3901 - val_acc: 0.4000\n",
      "Epoch 356/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.4751 - acc: 0.3390 - val_loss: 0.3899 - val_acc: 0.4000\n",
      "Epoch 357/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.4749 - acc: 0.3390 - val_loss: 0.3898 - val_acc: 0.4000\n",
      "Epoch 358/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4668 - acc: 0.360 - 0s 96us/step - loss: 0.4747 - acc: 0.3390 - val_loss: 0.3896 - val_acc: 0.4000\n",
      "Epoch 359/500\n",
      "59/59 [==============================] - 0s 130us/step - loss: 0.4745 - acc: 0.3390 - val_loss: 0.3894 - val_acc: 0.4000\n",
      "Epoch 360/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.4743 - acc: 0.3390 - val_loss: 0.3892 - val_acc: 0.4000\n",
      "Epoch 361/500\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.4741 - acc: 0.3390 - val_loss: 0.3891 - val_acc: 0.4000\n",
      "Epoch 362/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.4739 - acc: 0.3390 - val_loss: 0.3889 - val_acc: 0.4000\n",
      "Epoch 363/500\n",
      "59/59 [==============================] - 0s 118us/step - loss: 0.4737 - acc: 0.3390 - val_loss: 0.3887 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.4735 - acc: 0.3390 - val_loss: 0.3885 - val_acc: 0.4000\n",
      "Epoch 365/500\n",
      "59/59 [==============================] - 0s 127us/step - loss: 0.4732 - acc: 0.3390 - val_loss: 0.3883 - val_acc: 0.4000\n",
      "Epoch 366/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4730 - acc: 0.3390 - val_loss: 0.3881 - val_acc: 0.4000\n",
      "Epoch 367/500\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.4728 - acc: 0.3390 - val_loss: 0.3880 - val_acc: 0.4000\n",
      "Epoch 368/500\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.4726 - acc: 0.3390 - val_loss: 0.3878 - val_acc: 0.4000\n",
      "Epoch 369/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4724 - acc: 0.3390 - val_loss: 0.3875 - val_acc: 0.4000\n",
      "Epoch 370/500\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.4722 - acc: 0.3390 - val_loss: 0.3873 - val_acc: 0.4000\n",
      "Epoch 371/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.4720 - acc: 0.3390 - val_loss: 0.3871 - val_acc: 0.4000\n",
      "Epoch 372/500\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.4718 - acc: 0.3390 - val_loss: 0.3869 - val_acc: 0.4000\n",
      "Epoch 373/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4606 - acc: 0.380 - 0s 109us/step - loss: 0.4716 - acc: 0.3390 - val_loss: 0.3867 - val_acc: 0.4000\n",
      "Epoch 374/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.4714 - acc: 0.3390 - val_loss: 0.3865 - val_acc: 0.4000\n",
      "Epoch 375/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.4711 - acc: 0.3390 - val_loss: 0.3863 - val_acc: 0.4000\n",
      "Epoch 376/500\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.4709 - acc: 0.3390 - val_loss: 0.3862 - val_acc: 0.4000\n",
      "Epoch 377/500\n",
      "59/59 [==============================] - 0s 125us/step - loss: 0.4707 - acc: 0.3390 - val_loss: 0.3861 - val_acc: 0.4000\n",
      "Epoch 378/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.4705 - acc: 0.3390 - val_loss: 0.3859 - val_acc: 0.4000\n",
      "Epoch 379/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.4703 - acc: 0.3390 - val_loss: 0.3857 - val_acc: 0.4000\n",
      "Epoch 380/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.4701 - acc: 0.3390 - val_loss: 0.3855 - val_acc: 0.4000\n",
      "Epoch 381/500\n",
      "59/59 [==============================] - 0s 153us/step - loss: 0.4699 - acc: 0.3390 - val_loss: 0.3853 - val_acc: 0.4000\n",
      "Epoch 382/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4697 - acc: 0.3390 - val_loss: 0.3851 - val_acc: 0.4000\n",
      "Epoch 383/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.4695 - acc: 0.3390 - val_loss: 0.3848 - val_acc: 0.4000\n",
      "Epoch 384/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.4693 - acc: 0.3390 - val_loss: 0.3846 - val_acc: 0.4000\n",
      "Epoch 385/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.4691 - acc: 0.3390 - val_loss: 0.3843 - val_acc: 0.4000\n",
      "Epoch 386/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4690 - acc: 0.360 - 0s 85us/step - loss: 0.4688 - acc: 0.3390 - val_loss: 0.3841 - val_acc: 0.4000\n",
      "Epoch 387/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4686 - acc: 0.3390 - val_loss: 0.3838 - val_acc: 0.4000\n",
      "Epoch 388/500\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.4684 - acc: 0.3390 - val_loss: 0.3836 - val_acc: 0.4000\n",
      "Epoch 389/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4682 - acc: 0.3390 - val_loss: 0.3833 - val_acc: 0.4000\n",
      "Epoch 390/500\n",
      "59/59 [==============================] - 0s 137us/step - loss: 0.4680 - acc: 0.3390 - val_loss: 0.3830 - val_acc: 0.4000\n",
      "Epoch 391/500\n",
      "59/59 [==============================] - 0s 143us/step - loss: 0.4678 - acc: 0.3390 - val_loss: 0.3827 - val_acc: 0.4000\n",
      "Epoch 392/500\n",
      "59/59 [==============================] - 0s 120us/step - loss: 0.4675 - acc: 0.3390 - val_loss: 0.3824 - val_acc: 0.4000\n",
      "Epoch 393/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4673 - acc: 0.3390 - val_loss: 0.3822 - val_acc: 0.4000\n",
      "Epoch 394/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.4671 - acc: 0.3390 - val_loss: 0.3819 - val_acc: 0.4000\n",
      "Epoch 395/500\n",
      "59/59 [==============================] - 0s 124us/step - loss: 0.4669 - acc: 0.3390 - val_loss: 0.3817 - val_acc: 0.4000\n",
      "Epoch 396/500\n",
      "59/59 [==============================] - 0s 223us/step - loss: 0.4667 - acc: 0.3390 - val_loss: 0.3814 - val_acc: 0.4000\n",
      "Epoch 397/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.4665 - acc: 0.3390 - val_loss: 0.3812 - val_acc: 0.4000\n",
      "Epoch 398/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.4662 - acc: 0.3390 - val_loss: 0.3810 - val_acc: 0.4000\n",
      "Epoch 399/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.4660 - acc: 0.3390 - val_loss: 0.3808 - val_acc: 0.4000\n",
      "Epoch 400/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4658 - acc: 0.3390 - val_loss: 0.3806 - val_acc: 0.4000\n",
      "Epoch 401/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4656 - acc: 0.3390 - val_loss: 0.3804 - val_acc: 0.4000\n",
      "Epoch 402/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.4654 - acc: 0.3390 - val_loss: 0.3802 - val_acc: 0.4000\n",
      "Epoch 403/500\n",
      "59/59 [==============================] - 0s 138us/step - loss: 0.4652 - acc: 0.3390 - val_loss: 0.3800 - val_acc: 0.4000\n",
      "Epoch 404/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4650 - acc: 0.3390 - val_loss: 0.3797 - val_acc: 0.4000\n",
      "Epoch 405/500\n",
      "59/59 [==============================] - 0s 111us/step - loss: 0.4648 - acc: 0.3390 - val_loss: 0.3795 - val_acc: 0.4000\n",
      "Epoch 406/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4646 - acc: 0.3390 - val_loss: 0.3792 - val_acc: 0.4000\n",
      "Epoch 407/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4644 - acc: 0.3390 - val_loss: 0.3789 - val_acc: 0.4000\n",
      "Epoch 408/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.4641 - acc: 0.3390 - val_loss: 0.3786 - val_acc: 0.4000\n",
      "Epoch 409/500\n",
      "59/59 [==============================] - 0s 137us/step - loss: 0.4639 - acc: 0.3390 - val_loss: 0.3784 - val_acc: 0.4000\n",
      "Epoch 410/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4637 - acc: 0.3390 - val_loss: 0.3781 - val_acc: 0.4000\n",
      "Epoch 411/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4635 - acc: 0.3390 - val_loss: 0.3779 - val_acc: 0.4000\n",
      "Epoch 412/500\n",
      "59/59 [==============================] - 0s 85us/step - loss: 0.4633 - acc: 0.3390 - val_loss: 0.3776 - val_acc: 0.4000\n",
      "Epoch 413/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.4631 - acc: 0.3390 - val_loss: 0.3773 - val_acc: 0.4000\n",
      "Epoch 414/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4629 - acc: 0.3390 - val_loss: 0.3771 - val_acc: 0.4000\n",
      "Epoch 415/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4627 - acc: 0.3390 - val_loss: 0.3769 - val_acc: 0.4000\n",
      "Epoch 416/500\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.4625 - acc: 0.3390 - val_loss: 0.3767 - val_acc: 0.4000\n",
      "Epoch 417/500\n",
      "59/59 [==============================] - 0s 144us/step - loss: 0.4623 - acc: 0.3390 - val_loss: 0.3764 - val_acc: 0.4000\n",
      "Epoch 418/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4621 - acc: 0.3390 - val_loss: 0.3761 - val_acc: 0.4000\n",
      "Epoch 419/500\n",
      "59/59 [==============================] - 0s 104us/step - loss: 0.4619 - acc: 0.3390 - val_loss: 0.3759 - val_acc: 0.4000\n",
      "Epoch 420/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.4617 - acc: 0.3390 - val_loss: 0.3756 - val_acc: 0.4000\n",
      "Epoch 421/500\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.4615 - acc: 0.3390 - val_loss: 0.3753 - val_acc: 0.4000\n",
      "Epoch 422/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4613 - acc: 0.3390 - val_loss: 0.3751 - val_acc: 0.4000\n",
      "Epoch 423/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4611 - acc: 0.3390 - val_loss: 0.3749 - val_acc: 0.4000\n",
      "Epoch 424/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 91us/step - loss: 0.4609 - acc: 0.3390 - val_loss: 0.3747 - val_acc: 0.4000\n",
      "Epoch 425/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4607 - acc: 0.3390 - val_loss: 0.3745 - val_acc: 0.4000\n",
      "Epoch 426/500\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.4605 - acc: 0.3390 - val_loss: 0.3744 - val_acc: 0.4000\n",
      "Epoch 427/500\n",
      "59/59 [==============================] - 0s 96us/step - loss: 0.4603 - acc: 0.3390 - val_loss: 0.3742 - val_acc: 0.4000\n",
      "Epoch 428/500\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.4601 - acc: 0.3390 - val_loss: 0.3741 - val_acc: 0.4000\n",
      "Epoch 429/500\n",
      "59/59 [==============================] - 0s 109us/step - loss: 0.4599 - acc: 0.3390 - val_loss: 0.3740 - val_acc: 0.4000\n",
      "Epoch 430/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4649 - acc: 0.340 - 0s 84us/step - loss: 0.4597 - acc: 0.3390 - val_loss: 0.3738 - val_acc: 0.4000\n",
      "Epoch 431/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4595 - acc: 0.3390 - val_loss: 0.3736 - val_acc: 0.4000\n",
      "Epoch 432/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4593 - acc: 0.3390 - val_loss: 0.3735 - val_acc: 0.4000\n",
      "Epoch 433/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.4591 - acc: 0.3390 - val_loss: 0.3733 - val_acc: 0.4000\n",
      "Epoch 434/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4798 - acc: 0.300 - 0s 112us/step - loss: 0.4589 - acc: 0.3390 - val_loss: 0.3731 - val_acc: 0.4000\n",
      "Epoch 435/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.4587 - acc: 0.3390 - val_loss: 0.3728 - val_acc: 0.4000\n",
      "Epoch 436/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4585 - acc: 0.3390 - val_loss: 0.3726 - val_acc: 0.4000\n",
      "Epoch 437/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.4583 - acc: 0.3390 - val_loss: 0.3724 - val_acc: 0.4000\n",
      "Epoch 438/500\n",
      "59/59 [==============================] - 0s 75us/step - loss: 0.4581 - acc: 0.3390 - val_loss: 0.3721 - val_acc: 0.4000\n",
      "Epoch 439/500\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.4579 - acc: 0.3390 - val_loss: 0.3719 - val_acc: 0.4000\n",
      "Epoch 440/500\n",
      "59/59 [==============================] - 0s 130us/step - loss: 0.4576 - acc: 0.3390 - val_loss: 0.3716 - val_acc: 0.4000\n",
      "Epoch 441/500\n",
      "59/59 [==============================] - 0s 100us/step - loss: 0.4574 - acc: 0.3390 - val_loss: 0.3714 - val_acc: 0.4000\n",
      "Epoch 442/500\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.4572 - acc: 0.3390 - val_loss: 0.3711 - val_acc: 0.4000\n",
      "Epoch 443/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.4570 - acc: 0.3390 - val_loss: 0.3709 - val_acc: 0.4000\n",
      "Epoch 444/500\n",
      "59/59 [==============================] - 0s 145us/step - loss: 0.4568 - acc: 0.3390 - val_loss: 0.3706 - val_acc: 0.4000\n",
      "Epoch 445/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.4566 - acc: 0.3390 - val_loss: 0.3703 - val_acc: 0.4000\n",
      "Epoch 446/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4564 - acc: 0.3390 - val_loss: 0.3701 - val_acc: 0.4000\n",
      "Epoch 447/500\n",
      "59/59 [==============================] - 0s 101us/step - loss: 0.4562 - acc: 0.3390 - val_loss: 0.3699 - val_acc: 0.4000\n",
      "Epoch 448/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4560 - acc: 0.3390 - val_loss: 0.3696 - val_acc: 0.4000\n",
      "Epoch 449/500\n",
      "59/59 [==============================] - 0s 117us/step - loss: 0.4558 - acc: 0.3390 - val_loss: 0.3695 - val_acc: 0.4000\n",
      "Epoch 450/500\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4556 - acc: 0.3390 - val_loss: 0.3693 - val_acc: 0.4000\n",
      "Epoch 451/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.4555 - acc: 0.3390 - val_loss: 0.3691 - val_acc: 0.4000\n",
      "Epoch 452/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4553 - acc: 0.3390 - val_loss: 0.3689 - val_acc: 0.4000\n",
      "Epoch 453/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4551 - acc: 0.3390 - val_loss: 0.3688 - val_acc: 0.4000\n",
      "Epoch 454/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4549 - acc: 0.3390 - val_loss: 0.3686 - val_acc: 0.4000\n",
      "Epoch 455/500\n",
      "59/59 [==============================] - 0s 77us/step - loss: 0.4547 - acc: 0.3390 - val_loss: 0.3684 - val_acc: 0.4000\n",
      "Epoch 456/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4545 - acc: 0.3390 - val_loss: 0.3682 - val_acc: 0.4000\n",
      "Epoch 457/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4556 - acc: 0.340 - 0s 78us/step - loss: 0.4543 - acc: 0.3390 - val_loss: 0.3681 - val_acc: 0.4000\n",
      "Epoch 458/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.4542 - acc: 0.3390 - val_loss: 0.3679 - val_acc: 0.4000\n",
      "Epoch 459/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4540 - acc: 0.3390 - val_loss: 0.3677 - val_acc: 0.4000\n",
      "Epoch 460/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4538 - acc: 0.3390 - val_loss: 0.3676 - val_acc: 0.4000\n",
      "Epoch 461/500\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.4536 - acc: 0.3390 - val_loss: 0.3674 - val_acc: 0.4000\n",
      "Epoch 462/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.4534 - acc: 0.3390 - val_loss: 0.3672 - val_acc: 0.4000\n",
      "Epoch 463/500\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.4533 - acc: 0.3390 - val_loss: 0.3671 - val_acc: 0.4000\n",
      "Epoch 464/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.4531 - acc: 0.3390 - val_loss: 0.3669 - val_acc: 0.4000\n",
      "Epoch 465/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.4529 - acc: 0.3390 - val_loss: 0.3666 - val_acc: 0.4000\n",
      "Epoch 466/500\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4527 - acc: 0.3390 - val_loss: 0.3665 - val_acc: 0.4000\n",
      "Epoch 467/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4723 - acc: 0.300 - 0s 85us/step - loss: 0.4525 - acc: 0.3390 - val_loss: 0.3663 - val_acc: 0.4000\n",
      "Epoch 468/500\n",
      "59/59 [==============================] - 0s 133us/step - loss: 0.4524 - acc: 0.3390 - val_loss: 0.3660 - val_acc: 0.4000\n",
      "Epoch 469/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.4522 - acc: 0.3390 - val_loss: 0.3658 - val_acc: 0.4000\n",
      "Epoch 470/500\n",
      "59/59 [==============================] - 0s 80us/step - loss: 0.4520 - acc: 0.3390 - val_loss: 0.3655 - val_acc: 0.4000\n",
      "Epoch 471/500\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.4518 - acc: 0.3390 - val_loss: 0.3652 - val_acc: 0.4000\n",
      "Epoch 472/500\n",
      "59/59 [==============================] - 0s 72us/step - loss: 0.4517 - acc: 0.3390 - val_loss: 0.3650 - val_acc: 0.4000\n",
      "Epoch 473/500\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.4515 - acc: 0.3390 - val_loss: 0.3648 - val_acc: 0.4000\n",
      "Epoch 474/500\n",
      "59/59 [==============================] - 0s 116us/step - loss: 0.4513 - acc: 0.3390 - val_loss: 0.3646 - val_acc: 0.4000\n",
      "Epoch 475/500\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.4511 - acc: 0.3390 - val_loss: 0.3644 - val_acc: 0.4000\n",
      "Epoch 476/500\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.4510 - acc: 0.3390 - val_loss: 0.3642 - val_acc: 0.4000\n",
      "Epoch 477/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.4508 - acc: 0.3390 - val_loss: 0.3641 - val_acc: 0.4000\n",
      "Epoch 478/500\n",
      "59/59 [==============================] - 0s 105us/step - loss: 0.4506 - acc: 0.3390 - val_loss: 0.3639 - val_acc: 0.4000\n",
      "Epoch 479/500\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.4504 - acc: 0.3390 - val_loss: 0.3638 - val_acc: 0.4000\n",
      "Epoch 480/500\n",
      "59/59 [==============================] - 0s 113us/step - loss: 0.4503 - acc: 0.3390 - val_loss: 0.3637 - val_acc: 0.4000\n",
      "Epoch 481/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4501 - acc: 0.3390 - val_loss: 0.3635 - val_acc: 0.4000\n",
      "Epoch 482/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4515 - acc: 0.340 - 0s 136us/step - loss: 0.4499 - acc: 0.3390 - val_loss: 0.3633 - val_acc: 0.4000\n",
      "Epoch 483/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 100us/step - loss: 0.4497 - acc: 0.3390 - val_loss: 0.3632 - val_acc: 0.4000\n",
      "Epoch 484/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.4496 - acc: 0.3390 - val_loss: 0.3630 - val_acc: 0.4000\n",
      "Epoch 485/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4494 - acc: 0.3390 - val_loss: 0.3628 - val_acc: 0.4000\n",
      "Epoch 486/500\n",
      "59/59 [==============================] - 0s 131us/step - loss: 0.4492 - acc: 0.3390 - val_loss: 0.3627 - val_acc: 0.4000\n",
      "Epoch 487/500\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.4490 - acc: 0.3390 - val_loss: 0.3625 - val_acc: 0.4000\n",
      "Epoch 488/500\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.4489 - acc: 0.3390 - val_loss: 0.3623 - val_acc: 0.4000\n",
      "Epoch 489/500\n",
      "59/59 [==============================] - 0s 140us/step - loss: 0.4487 - acc: 0.3390 - val_loss: 0.3622 - val_acc: 0.4000\n",
      "Epoch 490/500\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4485 - acc: 0.3390 - val_loss: 0.3620 - val_acc: 0.4000\n",
      "Epoch 491/500\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4484 - acc: 0.3390 - val_loss: 0.3617 - val_acc: 0.4000\n",
      "Epoch 492/500\n",
      "59/59 [==============================] - 0s 112us/step - loss: 0.4482 - acc: 0.3390 - val_loss: 0.3615 - val_acc: 0.4000\n",
      "Epoch 493/500\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4480 - acc: 0.3390 - val_loss: 0.3613 - val_acc: 0.4000\n",
      "Epoch 494/500\n",
      "59/59 [==============================] - 0s 114us/step - loss: 0.4478 - acc: 0.3390 - val_loss: 0.3612 - val_acc: 0.4000\n",
      "Epoch 495/500\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.4477 - acc: 0.3390 - val_loss: 0.3610 - val_acc: 0.4000\n",
      "Epoch 496/500\n",
      "59/59 [==============================] - 0s 126us/step - loss: 0.4475 - acc: 0.3390 - val_loss: 0.3609 - val_acc: 0.4000\n",
      "Epoch 497/500\n",
      "59/59 [==============================] - 0s 137us/step - loss: 0.4473 - acc: 0.3390 - val_loss: 0.3608 - val_acc: 0.4000\n",
      "Epoch 498/500\n",
      "59/59 [==============================] - ETA: 0s - loss: 0.4309 - acc: 0.340 - 0s 98us/step - loss: 0.4472 - acc: 0.3390 - val_loss: 0.3606 - val_acc: 0.4000\n",
      "Epoch 499/500\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.4470 - acc: 0.3390 - val_loss: 0.3605 - val_acc: 0.4000\n",
      "Epoch 500/500\n",
      "59/59 [==============================] - 0s 116us/step - loss: 0.4468 - acc: 0.3390 - val_loss: 0.3604 - val_acc: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "hist = model.fit(x_train, y_train, nb_epoch=1000, batch_size=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.5889853239059448\n",
      "test acc: 0.4444444477558136\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEKCAYAAADNSVhkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncJJ8hBBCONz/A1smxf50S3p1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2256ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test acc:', score[1])\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(hist.history['loss'], linewidth=3, label='train')\n",
    "pyplot.plot(hist.history['val_loss'], linewidth=3, label='valid')\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.ylabel('loss')\n",
    "#pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale('log')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
